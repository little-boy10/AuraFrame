import { GoogleGenAI, Chat, GenerateContentResponse, Modality } from "@google/genai";
import { AspectRatio, CameraMovement, VideoGenerationStyle, VideoResolution } from "../types";

// FIX: Initialize the GoogleGenAI client.
// This client will be used for all API calls.
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

// --- Helper Functions ---
const fileToGenerativePart = async (file: File) => {
  const base64EncodedDataPromise = new Promise<string>((resolve) => {
    const reader = new FileReader();
    reader.onloadend = () => resolve((reader.result as string).split(',')[1]);
    reader.readAsDataURL(file);
  });
  return {
    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },
  };
};

const dataUrlToBase64 = (dataUrl: string) => {
    return dataUrl.split(',')[1];
}

// --- Image Studio Services ---

export const generateImage = async (prompt: string, aspectRatio: AspectRatio, numberOfImages: number): Promise<string> => {
    const response = await ai.models.generateImages({
        model: 'imagen-4.0-generate-001',
        prompt,
        config: {
            numberOfImages,
            aspectRatio,
            outputMimeType: 'image/png',
        },
    });
    
    const base64ImageBytes = response.generatedImages[0].image.imageBytes;
    return `data:image/png;base64,${base64ImageBytes}`;
};

export const editImage = async (imageDataUrl: string, prompt: string): Promise<string> => {
    const base64Image = dataUrlToBase64(imageDataUrl);
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image',
        contents: {
            parts: [
                {
                    inlineData: {
                        data: base64Image,
                        mimeType: 'image/png',
                    },
                },
                { text: prompt },
            ],
        },
        config: {
            responseModalities: [Modality.IMAGE],
        },
    });

    for (const part of response.candidates[0].content.parts) {
        if (part.inlineData) {
            const base64ImageBytes: string = part.inlineData.data;
            return `data:image/png;base64,${base64ImageBytes}`;
        }
    }
    throw new Error('No image was generated by the edit operation.');
};

export const analyzeImage = async (imageDataUrl: string): Promise<string> => {
    const base64Image = dataUrlToBase64(imageDataUrl);
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: {
            parts: [
                { text: 'Describe this image in detail.' },
                {
                    inlineData: {
                        data: base64Image,
                        mimeType: 'image/png',
                    },
                },
            ],
        },
    });
    return response.text;
};

export const upscaleImage = async (imageDataUrl: string): Promise<string> => {
    const base64Image = dataUrlToBase64(imageDataUrl);
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image',
        contents: {
            parts: [
                {
                    inlineData: {
                        data: base64Image,
                        mimeType: 'image/png',
                    },
                },
                { text: 'Upscale this image to a higher resolution, enhancing details, sharpness, and overall quality without changing the content.' },
            ],
        },
        config: {
            responseModalities: [Modality.IMAGE],
        },
    });

    for (const part of response.candidates[0].content.parts) {
        if (part.inlineData) {
            const base64ImageBytes: string = part.inlineData.data;
            return `data:image/png;base64,${base64ImageBytes}`;
        }
    }
    throw new Error('No upscaled image was generated.');
};


// --- Video Services ---
export const generateVideo = async (prompt: string, aspectRatio: AspectRatio, resolution: VideoResolution, visualStyle: VideoGenerationStyle, cameraMovement: CameraMovement) => {
    // A new AI instance must be created before each call to ensure the latest API key is used.
    const freshAi = new GoogleGenAI({ apiKey: process.env.API_KEY });
    
    const fullPrompt = `${prompt}, in a ${visualStyle} style. The camera shot should be a ${cameraMovement}.`;

    let operation = await freshAi.models.generateVideos({
        model: 'veo-3.1-fast-generate-preview',
        prompt: fullPrompt,
        config: {
            numberOfVideos: 1,
            resolution,
            aspectRatio,
        }
    });
    return operation;
};

export const extendVideo = async (previousOperation: any, prompt: string) => {
    // A new AI instance must be created before each call to ensure the latest API key is used.
    const freshAi = new GoogleGenAI({ apiKey: process.env.API_KEY });
    const previousVideo = previousOperation.response?.generatedVideos?.[0]?.video;

    if (!previousVideo) {
        throw new Error("Previous video data not found in the selected history item.");
    }

    // Veo extension requires 'veo-3.1-generate-preview' and 720p
    const resolution: VideoResolution = '720p';
    const aspectRatio = previousOperation.request?.config?.aspectRatio || '16:9';

    const operation = await freshAi.models.generateVideos({
      model: 'veo-3.1-generate-preview',
      prompt: prompt,
      video: previousVideo,
      config: {
        numberOfVideos: 1,
        resolution,
        aspectRatio,
      }
    });
    return operation;
};


export const checkVideoStatus = async (operation: any) => {
    // A new AI instance must be created before each call to ensure the latest API key is used.
    const freshAi = new GoogleGenAI({ apiKey: process.env.API_KEY });
    return await freshAi.operations.getVideosOperation({ operation: operation });
};


// --- Script/Text Services ---
export const generateScript = async (topic: string, audience: string, tone: string, length: string, platform: string): Promise<string> => {
    const prompt = `Write a video script about "${topic}". The target audience is ${audience}. The tone should be ${tone} and the length should be ${length}. The video is for the ${platform} platform. Include scene descriptions, dialogue/voiceover, and suggestions for visuals and sounds. Format it clearly with headings.`;
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-pro',
        contents: prompt,
    });
    return response.text;
};

export const generateViralStrategy = async (idea: string, videoFile?: File | null, audioFile?: File | null, competitorUrl?: string): Promise<string> => {
    const parts: any[] = [{ text: `
        Analyze the following content idea and generate a comprehensive viral marketing strategy for YouTube.
        Core Idea: "${idea}"
        ${competitorUrl ? `Competitor analysis: Analyze this successful video and identify key viral triggers: ${competitorUrl}` : ''}
        
        Provide the following, formatted in Markdown:
        - **Catchy Titles:** 5 options combining curiosity, urgency, and keywords.
        - **Compelling Description:** A paragraph optimized for SEO with a strong hook.
        - **Viral Hashtags:** A mix of broad and niche hashtags.
        - **Thumbnail Concepts:** 3 distinct, high-contrast, emotion-driven ideas for a thumbnail.
        - **Growth Hacks:** 3 actionable tips to boost initial traction (e.g., community engagement, collaborations, short-form content repurposing).
    `}];

    if (videoFile) {
        parts.push(await fileToGenerativePart(videoFile));
        parts.push({ text: "\nAlso analyze the provided video content for strengths and weaknesses." });
    }
    if (audioFile) {
        parts.push(await fileToGenerativePart(audioFile));
        parts.push({ text: "\nAlso analyze the provided audio for tone, clarity, and engagement." });
    }

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-pro',
        contents: { parts },
    });
    return response.text;
};


// --- Audio Services ---
export const generateSpeech = async (text: string, voiceName: string, voiceDescription?: string): Promise<string> => {
    // Process text for pause syntax, converting [pause:500ms] into a natural language instruction.
    const processedText = text.replace(/\[pause:(\d+)ms\]/g, (_, milliseconds) => {
        return ` (pause for ${milliseconds} milliseconds) `;
    }).replace(/\s+/g, ' ').trim();

    let prompt = processedText;
    if (voiceDescription) {
        // This is a way to guide the model for "custom" voices.
        prompt = `Using a voice that sounds like this: "${voiceDescription}", say the following: ${processedText}`;
    }

    const response = await ai.models.generateContent({
        model: "gemini-2.5-flash-preview-tts",
        contents: [{ parts: [{ text: prompt }] }],
        config: {
            responseModalities: [Modality.AUDIO],
            speechConfig: {
                voiceConfig: {
                    prebuiltVoiceConfig: { voiceName },
                },
            },
        },
    });

    const base64Audio = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
    if (!base64Audio) {
        throw new Error('Failed to generate audio data.');
    }
    return base64Audio;
};


// --- Chat Services ---
export const createChatSession = (): Chat => {
    const chat: Chat = ai.chats.create({
        model: 'gemini-2.5-flash',
        config: {
            systemInstruction: 'You are a helpful assistant for a creative content generation application called AuraFrame Studio. Be concise and helpful.',
        },
    });
    return chat;
};
